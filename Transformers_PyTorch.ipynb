{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4bltvTgPPqKk",
        "Oj2CKqE-P1Nm"
      ],
      "authorship_tag": "ABX9TyMFr5gWEATiy9n4/b2SBlwM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prithuls/Anomaly-Transformer/blob/main/Transformers_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Vision Transformer**"
      ],
      "metadata": {
        "id": "4bltvTgPPqKk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYel7sKheSZn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels= 3, embed_dim= 768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels,\n",
        "            embed_dim,\n",
        "            kernel_size= patch_size,\n",
        "            stride= patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x) ## n_sample, embed_dim, patch_size, patch_size\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "txfkdPNG7xMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads= 12, qkv_bias= True, attn_p= 0., proj_p= 0.):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // n_heads\n",
        "        self.scale = self.head_dim ** (-0.5)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias= qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_p)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_samples, n_tokens, dim = x.shape\n",
        "        if dim != self.dim:\n",
        "            raise ValueError\n",
        "        qkv = self.qkv(x) ## n_samples, n_tokens, 3 * dim\n",
        "        qkv = qkv.reshape(\n",
        "            n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
        "        )\n",
        "        qkv = qkv.permute(\n",
        "            2, 0, 3, 1, 4\n",
        "        )   ## 3, n_samples, n_heads, n_tokens, head_dim\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2] ## n_samples, n_heads, n_tokens, head_dim\n",
        "        k_t = k.transpose(-2, -1)\n",
        "\n",
        "        dp = (q @ k_t) * self.scale ## n_samples, n_heads, n_tokens, n_tokens\n",
        "        attn = dp.softmax(dim = -1) ## n_samples, n_heads, n_tokens, n_tokens\n",
        "        attn = self.attn_drop(attn) \n",
        "\n",
        "        weighted_avg = attn @ v ## n_samples, n_heads, n_tokens, head_dim\n",
        "        weighted_avg = weighted_avg.transpose(1, 2) ## n_samples, n_tokens, n_heads, head_dim\n",
        "\n",
        "        weighted_avg = weighted_avg.flatten(2) ## n_samples, n_tokens, dim\n",
        "        x = self.proj(weighted_avg)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jZ4HcwrNAfQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features, p= 0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Runs forward pass\n",
        "\n",
        "\n",
        "        Paremeters\n",
        "        ----------\n",
        "        x ## (n_samples, n_tokens, in_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor ## (n_samples, n_tokens, out_features)\n",
        "        \n",
        "        \"\"\"\n",
        "        x = self.fc1(x) ## n_samples, n_tokens, hidden_features\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x) ## n_samples, n_tokens, out_features\n",
        "        x = self.drop(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "0e4YduLxwE3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module): \n",
        "    def __init__(self, dim, n_heads, mlp_ratio= 4.0, qkv_bias= True, p= 0., attn_p= 0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim, eps= 1e-6)\n",
        "        self.attn = Attention(\n",
        "            dim, \n",
        "            n_heads = n_heads, \n",
        "            qkv_bias = qkv_bias, \n",
        "            attn_p = attn_p, \n",
        "            proj_p = p\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(dim, eps= 1e-6)\n",
        "        hidden_features = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(\n",
        "            in_features = dim,\n",
        "            hidden_features = hidden_features,\n",
        "            out_features = dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "O6U4n-QqAgrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        img_size= 256,\n",
        "        patch_size= 16,\n",
        "        in_channels= 3,\n",
        "        n_classes= 1000,\n",
        "        embed_dim= 768,\n",
        "        depth= 12,\n",
        "        n_heads= 12,\n",
        "        mlp_ratio= 4,\n",
        "        qkv_bias= True,\n",
        "        p= 0,\n",
        "        attn_p= 0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.patch_embed= PatchEmbed(\n",
        "            img_size= img_size,\n",
        "            patch_size= patch_size, \n",
        "            in_channels= in_channels, \n",
        "            embed_dim= embed_dim\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim)\n",
        "        )\n",
        "        self.pos_drop = nn.Dropout(p= p)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    dim= embed_dim, \n",
        "                    n_heads= n_heads, \n",
        "                    mlp_ratio= mlp_ratio, \n",
        "                    qkv_bias= qkv_bias, \n",
        "                    p= p, \n",
        "                    attn_p= attn_p\n",
        "                )\n",
        "                for _ in range(depth)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim, eps= 1e-6)\n",
        "        self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_samples = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_token = self.cls_token.expand(\n",
        "            n_samples, -1, -1 \n",
        "        )\n",
        "        x = torch.cat((cls_token, x), dim= 1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        x = self.norm(x)\n",
        "        cls_token_final = x[:,0]\n",
        "        x = self.head(cls_token_final)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "-d5RF5ywD_BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "diWKZNcrP03v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Swin Transformer**"
      ],
      "metadata": {
        "id": "Oj2CKqE-P1Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl-IUt3Fjk1F",
        "outputId": "07196986-bea5-40d7-bfdf-4923f877d35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat"
      ],
      "metadata": {
        "id": "0m7TOGjMP4uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## For shifting window\n",
        "\n",
        "class CyclicShift(nn.Module):\n",
        "    def __init__(self, displacement):\n",
        "        super().__init__()\n",
        "        self.displacement = displacement\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return torch.roll(x, shifts= (self.displacement, self.displacement), dims= (1, 2))\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x"
      ],
      "metadata": {
        "id": "5bSqUZFOQD59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )  \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "qk4zew_jQELi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This helps with shifted windows\n",
        "\n",
        "def create_mask(window_size, displacement, upper_lower, left_right):\n",
        "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
        "\n",
        "    if upper_lower:\n",
        "        mask[- displacement * window_size : , : - displacement * window_size] = float('-inf')\n",
        "        mask[: - displacement * window_size, - displacement * window_size : ] = float('-inf')\n",
        "\n",
        "    if left_right:\n",
        "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2',\n",
        "                                h1 = window_size, h2 = window_size)\n",
        "        mask[:, - displacement :, :, : - displacement] = float(\"-inf\")\n",
        "        mask[:, : - displacement, :, -displacement : ] = float(\"-inf\")\n",
        "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
        "\n",
        "    return mask\n"
      ],
      "metadata": {
        "id": "7lbsSAkyQFbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## For distances between two patches / windows\n",
        "\n",
        "def get_relative_distances(window_size):\n",
        "    indices = torch.tensor(\n",
        "        np.array(\n",
        "            [[x, y] for x in range(window_size) for y in range(window_size)]\n",
        "        )\n",
        "    )\n",
        "    distances = indices[None, :, :] - indices[:, None, :]\n",
        "    return distances"
      ],
      "metadata": {
        "id": "LXUELtY9QFeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = torch.tensor(\n",
        "      np.array(\n",
        "          [[x, y] for x in range(2) for y in range(2)]\n",
        "      )\n",
        "  )\n",
        "distances = indices[None, :, :] - indices[:, None, :]"
      ],
      "metadata": {
        "id": "ywLVUIBcmRXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads,\n",
        "        head_dim,\n",
        "        shifted,\n",
        "        window_size,\n",
        "        relative_pos_embedding\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = head_dim * heads\n",
        "        self.heads = heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.window_size = window_size\n",
        "        self.relative_pos_embedding = relative_pos_embedding\n",
        "        self.shifted = shifted\n",
        "\n",
        "        if self.shifted:\n",
        "            displacement = window_size // 2\n",
        "            self.cyclic_shift = CyclicShift(-displacement)\n",
        "            self.cyclic_back_shift = CyclicShift(displacement)\n",
        "\n",
        "            self.upper_lower_mask = nn.Parameter(\n",
        "                create_mask(\n",
        "                    window_size = window_size,\n",
        "                    displacement = displacement,\n",
        "                    upper_lower = True,\n",
        "                    left_right = False\n",
        "                ), requires_grad= False\n",
        "            )\n",
        "\n",
        "            self.left_right_mask = nn.Parameter(\n",
        "                create_mask(\n",
        "                    window_size = window_size,\n",
        "                    displacement = displacement,\n",
        "                    upper_lower = False,\n",
        "                    left_right = True\n",
        "                ), requires_grad= False\n",
        "            )\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias= False)\n",
        "\n",
        "        if self.relative_pos_embedding:\n",
        "            self.relative_indices = get_relative_distances(window_size) + window_size - 1 ## Values get positive here\n",
        "            self.pos_embedding = nn.Parameter(\n",
        "                torch.randn(\n",
        "                    2 * window_size - 1, 2 * window_size - 1\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
        "\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.shifted:\n",
        "            x = self.cyclic_shift(x)\n",
        "        \n",
        "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim= -1)\n",
        "        nw_h = n_h // self.window_size\n",
        "        nw_w = n_w // self.window_size\n",
        "\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
        "                                h = h, w_h = self.window_size, w_w = self.window_size,), qkv\n",
        "        )\n",
        "\n",
        "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
        "        if self.relative_pos_embedding:\n",
        "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
        "        else:\n",
        "            dots += self.pos_embedding\n",
        "\n",
        "        if self.shifted:\n",
        "            dots[:, :, -nw_w : ] += self.upper_lower_mask\n",
        "            dots[:, :, nw_w - 1 :: nw_w] += self.left_right_mask\n",
        "\n",
        "        attn = dots.softmax(dim = -1)\n",
        "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
        "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
        "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
        "        \n",
        "        out = self.to_out(out)\n",
        "\n",
        "        if self.shifted:\n",
        "            out = self.cyclic_back_shift(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "A74wMj5pQFg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads,\n",
        "        head_dim,\n",
        "        mlp_dim, \n",
        "        shifted,\n",
        "        window_size,\n",
        "        relative_pos_embedding\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.attention_block = Residual(\n",
        "            PreNorm(dim, \n",
        "                WindowAttention(\n",
        "                    dim= dim,\n",
        "                    heads= heads,\n",
        "                    head_dim= head_dim,\n",
        "                    shifted= shifted,\n",
        "                    window_size= window_size,\n",
        "                    relative_pos_embedding= relative_pos_embedding\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        self.mlp_block = Residual(\n",
        "            PreNorm(dim, FeedForward(dim= dim, hidden_dim= mlp_dim))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention_block(x)\n",
        "        x = self.mlp_block(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "eU2J8gygjZQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "        super().__init__()\n",
        "        self.downscaling_factor = downscaling_factor\n",
        "        self.patch_merge = nn.Unfold(\n",
        "            kernel_size= downscaling_factor,\n",
        "            stride= downscaling_factor,\n",
        "            padding= 0\n",
        "        )\n",
        "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
        "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "nVdiOOEYQFkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StageModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        hidden_dimension,\n",
        "        layers,\n",
        "        downscaling_factor, \n",
        "        num_heads,\n",
        "        head_dim,\n",
        "        window_size,\n",
        "        relative_pos_embedding\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert layers % 2 == 0\n",
        "        self.patch_partition = PatchMerging(\n",
        "            in_channels= in_channels,\n",
        "            out_channels= hidden_dimension,\n",
        "            downscaling_factor= downscaling_factor\n",
        "        )\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(layers // 2):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                SwinBlock(\n",
        "                    dim= hidden_dimension,\n",
        "                    heads= num_heads,\n",
        "                    head_dim= head_dim,\n",
        "                    mlp_dim= hidden_dimension * 4,\n",
        "                    shifted= False,\n",
        "                    window_size= window_size,\n",
        "                    relative_pos_embedding= relative_pos_embedding\n",
        "                ),\n",
        "                SwinBlock(\n",
        "                    dim= hidden_dimension,\n",
        "                    heads= num_heads,\n",
        "                    head_dim= head_dim,\n",
        "                    mlp_dim= hidden_dimension * 4,\n",
        "                    shifted= True,\n",
        "                    window_size= window_size,\n",
        "                    relative_pos_embedding= relative_pos_embedding\n",
        "                \n",
        "                )\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_partition(x)\n",
        "        for regular_block, shifted_block in self.layers:\n",
        "            x = regular_block(x)\n",
        "            x = shifted_block(x)\n",
        "        \n",
        "\n",
        "        return x.permute(0, 3, 1, 2)"
      ],
      "metadata": {
        "id": "44ouL4WzQFnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        hidden_dim,\n",
        "        layers,\n",
        "        heads,\n",
        "        channels= 3,\n",
        "        num_classes= 1000,\n",
        "        head_dim= 32,\n",
        "        window_size= 7,\n",
        "        downscaling_factor= (4, 2, 2, 2),\n",
        "        relative_pos_embedding= True\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stage1 = StageModule(\n",
        "            in_channels= channels,\n",
        "            hidden_dimension= hidden_dim,\n",
        "            layers= layers[0],\n",
        "            downscaling_factor= downscaling_factor[0],\n",
        "            num_heads= heads[0],\n",
        "            head_dim= head_dim,\n",
        "            window_size= window_size,\n",
        "            relative_pos_embedding= relative_pos_embedding\n",
        "        )\n",
        "\n",
        "        self.stage2 = StageModule(\n",
        "            in_channels= hidden_dim,\n",
        "            hidden_dimension= hidden_dim * 2,\n",
        "            layers= layers[1],\n",
        "            downscaling_factor= downscaling_factor[1],\n",
        "            num_heads= heads[1],\n",
        "            head_dim= head_dim,\n",
        "            window_size= window_size,\n",
        "            relative_pos_embedding= relative_pos_embedding\n",
        "        )\n",
        "\n",
        "        self.stage3 = StageModule(\n",
        "            in_channels= hidden_dim * 2,\n",
        "            hidden_dimension= hidden_dim * 4,\n",
        "            layers= layers[2],\n",
        "            downscaling_factor= downscaling_factor[2],\n",
        "            num_heads= heads[2],\n",
        "            head_dim= head_dim,\n",
        "            window_size= window_size,\n",
        "            relative_pos_embedding= relative_pos_embedding\n",
        "        )\n",
        "\n",
        "        self.stage4 = StageModule(\n",
        "            in_channels= hidden_dim * 4,\n",
        "            hidden_dimension= hidden_dim * 8,\n",
        "            layers= layers[3],\n",
        "            downscaling_factor= downscaling_factor[3],\n",
        "            num_heads= heads[3],\n",
        "            head_dim= head_dim,\n",
        "            window_size= window_size,\n",
        "            relative_pos_embedding= relative_pos_embedding\n",
        "        )\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.stage1(img)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "\n",
        "        x = x.mean(dim= [2, 3])\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "ursPPF7MQFqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = SwinTransformer(\n",
        "    hidden_dim=96,\n",
        "    layers=(2, 2, 6, 2),\n",
        "    heads=(3, 6, 12, 24),\n",
        "    channels=3,\n",
        "    num_classes=3,\n",
        "    head_dim=32,\n",
        "    window_size=7,\n",
        "    downscaling_factor=(4, 2, 2, 2),\n",
        "    relative_pos_embedding=True\n",
        ")\n",
        "dummy_x = torch.randn(1, 3, 224, 224)"
      ],
      "metadata": {
        "id": "KNCSbTEtQFuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = net(dummy_x)  # (1,3)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "a9_MH84x87nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "K4OS5_saAkas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "babf0259-f46b-45ce-be6b-9fea37976875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "0LBSQeLc-d7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(net, input_size=(32, 3, 224, 224), col_names= [\"input_size\", \"output_size\", \"num_params\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgs90dsVAi3M",
        "outputId": "d022a502-6853-4367-ecf8-287d09de2c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=======================================================================================================================================\n",
              "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #\n",
              "=======================================================================================================================================\n",
              "SwinTransformer                                              [32, 3, 224, 224]         [32, 3]                   --\n",
              "├─StageModule: 1-1                                           [32, 3, 224, 224]         [32, 96, 56, 56]          --\n",
              "│    └─PatchMerging: 2-1                                     [32, 3, 224, 224]         [32, 56, 56, 96]          --\n",
              "│    │    └─Unfold: 3-1                                      [32, 3, 224, 224]         [32, 48, 3136]            --\n",
              "│    │    └─Linear: 3-2                                      [32, 56, 56, 48]          [32, 56, 56, 96]          4,704\n",
              "│    └─ModuleList: 2-2                                       --                        --                        --\n",
              "│    │    └─ModuleList: 3-3                                  --                        --                        228,244\n",
              "├─StageModule: 1-2                                           [32, 96, 56, 56]          [32, 192, 28, 28]         --\n",
              "│    └─PatchMerging: 2-3                                     [32, 96, 56, 56]          [32, 28, 28, 192]         --\n",
              "│    │    └─Unfold: 3-4                                      [32, 96, 56, 56]          [32, 384, 784]            --\n",
              "│    │    └─Linear: 3-5                                      [32, 28, 28, 384]         [32, 28, 28, 192]         73,920\n",
              "│    └─ModuleList: 2-4                                       --                        --                        --\n",
              "│    │    └─ModuleList: 3-6                                  --                        --                        893,716\n",
              "├─StageModule: 1-3                                           [32, 192, 28, 28]         [32, 384, 14, 14]         --\n",
              "│    └─PatchMerging: 2-5                                     [32, 192, 28, 28]         [32, 14, 14, 384]         --\n",
              "│    │    └─Unfold: 3-7                                      [32, 192, 28, 28]         [32, 768, 196]            --\n",
              "│    │    └─Linear: 3-8                                      [32, 14, 14, 768]         [32, 14, 14, 384]         295,296\n",
              "│    └─ModuleList: 2-6                                       --                        --                        --\n",
              "│    │    └─ModuleList: 3-9                                  --                        --                        3,551,764\n",
              "│    │    └─ModuleList: 3-10                                 --                        --                        3,551,764\n",
              "│    │    └─ModuleList: 3-11                                 --                        --                        3,551,764\n",
              "├─StageModule: 1-4                                           [32, 384, 14, 14]         [32, 768, 7, 7]           --\n",
              "│    └─PatchMerging: 2-7                                     [32, 384, 14, 14]         [32, 7, 7, 768]           --\n",
              "│    │    └─Unfold: 3-12                                     [32, 384, 14, 14]         [32, 1536, 49]            --\n",
              "│    │    └─Linear: 3-13                                     [32, 7, 7, 1536]          [32, 7, 7, 768]           1,180,416\n",
              "│    └─ModuleList: 2-8                                       --                        --                        --\n",
              "│    │    └─ModuleList: 3-14                                 --                        --                        14,176,276\n",
              "├─Sequential: 1-5                                            [32, 768]                 [32, 3]                   --\n",
              "│    └─LayerNorm: 2-9                                        [32, 768]                 [32, 768]                 1,536\n",
              "│    └─Linear: 2-10                                          [32, 768]                 [32, 3]                   2,307\n",
              "=======================================================================================================================================\n",
              "Total params: 27,511,707\n",
              "Trainable params: 27,482,895\n",
              "Non-trainable params: 28,812\n",
              "Total mult-adds (M): 879.39\n",
              "=======================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 4171.63\n",
              "Params size (MB): 109.92\n",
              "Estimated Total Size (MB): 4300.82\n",
              "======================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.module import _addindent\n",
        "import torch\n",
        "import numpy as np\n",
        "def torch_summarize(model, show_weights=True, show_parameters=True):\n",
        "    \"\"\"Summarizes torch model by showing trainable parameters and weights.\"\"\"\n",
        "    tmpstr = model.__class__.__name__ + ' (\\n'\n",
        "    for key, module in model._modules.items():\n",
        "        # if it contains layers let call it recursively to get params and weights\n",
        "        if type(module) in [\n",
        "            torch.nn.modules.container.Container,\n",
        "            torch.nn.modules.container.Sequential\n",
        "        ]:\n",
        "            modstr = torch_summarize(module)\n",
        "        else:\n",
        "            modstr = module.__repr__()\n",
        "        modstr = _addindent(modstr, 2)\n",
        "\n",
        "        params = sum([np.prod(p.size()) for p in module.parameters()])\n",
        "        weights = tuple([tuple(p.size()) for p in module.parameters()])\n",
        "\n",
        "        tmpstr += '  (' + key + '): ' + modstr \n",
        "        if show_weights:\n",
        "            tmpstr += ', weights={}'.format(weights)\n",
        "        if show_parameters:\n",
        "            tmpstr +=  ', parameters={}'.format(params)\n",
        "        tmpstr += '\\n'   \n",
        "\n",
        "    tmpstr = tmpstr + ')'\n",
        "    return tmpstr"
      ],
      "metadata": {
        "id": "T2AgOBYeAzKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch_summarize(net))"
      ],
      "metadata": {
        "id": "nt8IIZeyEmBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_shape(model, image_dim):\n",
        "    return model(torch.rand(*(image_dim))).data.shape"
      ],
      "metadata": {
        "id": "Co7agVTzEp9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_output_shape(net, (32, 3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6CQ4RZYFKU8",
        "outputId": "0bd6bf95-810e-4bf1-8a42-3a3383c9dabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kR7cDO6ExSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfYXQm0HExP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StageModule(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
        "\n",
        "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
        "                                            downscaling_factor=downscaling_factor)\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(layers // 2):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_partition(x)\n",
        "        for regular_block, shifted_block in self.layers:\n",
        "            x = regular_block(x)\n",
        "            x = shifted_block(x)\n",
        "        return x.permute(0, 3, 1, 2)"
      ],
      "metadata": {
        "id": "tcTGai9OExNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stageM = StageModule(\n",
        "    in_channels, \n",
        "    hidden_dimension, \n",
        "    layers, \n",
        "    downscaling_factor, \n",
        "    num_heads, \n",
        "    head_dim, \n",
        "    window_size,\n",
        "    relative_pos_embedding\n",
        ")"
      ],
      "metadata": {
        "id": "c5lfUjm7Ex6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vi68k6pEGPaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "        super().__init__()\n",
        "        self.downscaling_factor = downscaling_factor\n",
        "        self.patch_merge = nn.Unfold(\n",
        "            kernel_size= downscaling_factor,\n",
        "            stride= downscaling_factor,\n",
        "            padding= 0\n",
        "        )\n",
        "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
        "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "0jFXiqcAGPUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = PatchMerging(in_channels=3, out_channels=32,\n",
        "                  downscaling_factor=4)"
      ],
      "metadata": {
        "id": "yeu4SPMyGRKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(net, input_size=(32, 3, 224, 224), col_names= [\"input_size\", \"output_size\", \"num_params\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YAqNlgRGZhY",
        "outputId": "2cc3f840-6a5d-40c1-e5d5-af024845e0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===================================================================================================================\n",
              "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
              "===================================================================================================================\n",
              "PatchMerging                             [32, 3, 224, 224]         [32, 56, 56, 32]          --\n",
              "├─Unfold: 1-1                            [32, 3, 224, 224]         [32, 48, 3136]            --\n",
              "├─Linear: 1-2                            [32, 56, 56, 48]          [32, 56, 56, 32]          1,568\n",
              "===================================================================================================================\n",
              "Total params: 1,568\n",
              "Trainable params: 1,568\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.05\n",
              "===================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 25.69\n",
              "Params size (MB): 0.01\n",
              "Estimated Total Size (MB): 44.96\n",
              "==================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdD4CfjtNkmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GTN"
      ],
      "metadata": {
        "id": "1Rj-ZtpDFazc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kh2PKUHgFc1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anomaly Transformer"
      ],
      "metadata": {
        "id": "KEP2W8PUQ2Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import os"
      ],
      "metadata": {
        "id": "IFdpm22iQ7DK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TriangularCausalMask():\n",
        "    def __init__(self, B, L, device= 'cpu'):\n",
        "        mask_shape = [B, 1, L, L]\n",
        "        with torch.no_grad():\n",
        "            self._mask = torch.triu(\n",
        "                torch.ones(\n",
        "                    mask_shape, dtype= torch.bool\n",
        "                ), diagonal= 1\n",
        "            ).to(device)\n",
        "        \n",
        "    @property\n",
        "    def mask(self):\n",
        "        return self.mask\n",
        "\n",
        "\n",
        "class AnomalyAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        win_size, \n",
        "        mask_flag= True,\n",
        "        scale= None,\n",
        "        attention_dropout= 0.0,\n",
        "        output_attention= False\n",
        "    ):\n",
        "        super(AnomalyAttention, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "        window_size = win_size\n",
        "        self.distances = torch.zeros((window_size, window_size)).cuda()\n",
        "        for i in range(window_size):\n",
        "            for j in range(window_size):\n",
        "                self.distances[i][j] = abs(i - j)\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        queries,\n",
        "        keys,\n",
        "        values, \n",
        "        sigma,\n",
        "        attn_mask\n",
        "    ): \n",
        "        B, L, H, E = queries.shape    ## Batch_size, \n",
        "        _, S, _, D = values.shape  ## Isn't both the queries and values same shape??\n",
        "        scale = self.scale or 1. / math.sqrt(E)\n",
        "\n",
        "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
        "        if self.mask_flag:\n",
        "            if attn_mask is None:\n",
        "                attn_mask = TriangularCausalMask(B, L, device= queries.device)\n",
        "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
        "        attn = scale * scores\n",
        "\n",
        "        sigma = sigma.transpose(1, 2) # B L H ->  B H L\n",
        "        window_size = attn.shape[-1]\n",
        "        sigma = torch.sigmoid(sigma * 5) + 1e-5\n",
        "        sigma = torch.pow(3, sigma) - 1\n",
        "        sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, window_size)\n",
        "        prior = self.distances.unsqueeze(0).unsqueeze(0).repeat(\n",
        "                    sigma.shape[0], sigma.shape[1], 1, 1\n",
        "                ).cuda()\n",
        "        prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / (2 * sigma ** 2))\n",
        "\n",
        "        series = self.dropout(torch.softmax(attn, dim= 1))\n",
        "        V = torch.einsum(\"bhls,bshd->blhd\", series, values)\n",
        "        if self.output_attention:\n",
        "            return (V.contiguous(), series, prior, sigma)\n",
        "        else:\n",
        "            return (V.contiguous(), None)\n"
      ],
      "metadata": {
        "id": "M6w6pRvzRQ7a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention,\n",
        "        d_model,\n",
        "        n_heads,\n",
        "        d_keys= None,\n",
        "        d_values= None\n",
        "    ):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        d_keys = d_keys or (d_model // n_heads)\n",
        "        d_values = d_values or (d_model // n_heads)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.inner_attention = attention\n",
        "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
        "        self.sigma_projection = nn.Linear(d_model, n_heads)\n",
        "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        queries,\n",
        "        keys,\n",
        "        values, \n",
        "        attn_mask\n",
        "    ):\n",
        "        B, L, _ = queries.shape\n",
        "        _, S, _ = keys.shape\n",
        "        H = self.n_heads\n",
        "        x = queries\n",
        "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
        "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
        "        values = self.value_projection(values).view(B, S, H, -1)\n",
        "        sigma = self.sigma_projection(x).view(B, L, H)\n",
        "\n",
        "        out, series, prior, sigma = self.inner_attention(\n",
        "            queries,\n",
        "            keys,\n",
        "            values, \n",
        "            sigma,\n",
        "            attn_mask\n",
        "        )\n",
        "        out = out.view(B, L, -1)\n",
        "        return self.out_projection(out), series, prior, sigma"
      ],
      "metadata": {
        "id": "MKvOhFv0WAlZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention,\n",
        "        d_model,\n",
        "        d_ff= None,\n",
        "        dropout= 0.1,\n",
        "        activation= 'relu'\n",
        "    ):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        d_ff = d_ff or d_model * 4\n",
        "        self.attention = attention\n",
        "        self.conv1 = nn.Conv1d(in_channels= d_model, out_channel= d_ff, kernel_size= 1)\n",
        "        self.conv2 = nn.Conv1d(in_channels= d_ff, out_channel= d_model, kernel_size= 1)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == 'relu' else F.gelu\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        x,\n",
        "        attn_mask= None\n",
        "    ):  \n",
        "        new_x, attn, mask, sigma = self.attention(\n",
        "            x, x, x, attn_mask= attn_mask\n",
        "        )\n",
        "        x = x + self.dropout(new_x)\n",
        "        y = x = self.norm1(x)\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
        "        return self.norm2(x + y), attn, mask, sigma\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_layers,\n",
        "        norm_layer= None\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attn_layers = nn.ModuleList(attn_layers)\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        x, \n",
        "        attn_mask\n",
        "    ):\n",
        "        series_list = []\n",
        "        prior_list = []\n",
        "        sigma_list = []\n",
        "\n",
        "        for attn_layer in self.attn_layers:\n",
        "            x, series, prior, sigma = attn_layer(x, attn_mask= attn_mask)\n",
        "            series_list.append(series)\n",
        "            prior_list.append(prior)\n",
        "            sigma_list.append(sigma)\n",
        "\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        \n",
        "        return x, series_list, prior_list, sigma_list\n",
        "\n",
        "class AnomalyTransfomer(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        \n",
        "    )"
      ],
      "metadata": {
        "id": "dodfKCP4WAiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vd7DlHJgWAf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyAGm9_gWAdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AtqhtgGWAaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "638umRfRWAXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyeQZll5WAVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "syO9UndoWASR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QQjqI-6fWAPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Xmm-mWhWANA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W_AU5N8tWAKJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}